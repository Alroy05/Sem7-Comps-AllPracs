{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NLP - EXP - 2\n",
        "\n",
        "Atharva Prashant Pawar (9427) - [ Batch - D ]"
      ],
      "metadata": {
        "id": "XHjg0fJ5uqf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a python code to remove punctuations, URLs and stop words."
      ],
      "metadata": {
        "id": "bp7J8Bq0ZgLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd0RORRUZdH8",
        "outputId": "cf56dc57-eed9-4de3-e2da-de8270160f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Raw Text :  Hello, world! This is a sample text with a URL: https://www.myname.com\n",
            "## Remove_punctuations from text :  Hello world This is a sample text with a URL httpswwwmynamecom\n",
            "## Remove_urls from text :  Hello, world! This is a sample text with a URL: \n",
            "## Remove_stop_words from text :  Hello, world! sample text URL: https://www.myname.com\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def remove_punctuations(text): # Remove punctuations\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def remove_urls(text): # Remove URLs\n",
        "    return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
        "\n",
        "def remove_stop_words(text): # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = text.split()\n",
        "    filtered_words = []\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            filtered_words.append(word)\n",
        "    return (\" \".join(filtered_words), stop_words)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    print(\"## Remove_punctuations from text : \", remove_punctuations(text))\n",
        "    print(\"## Remove_urls from text : \", remove_urls(text))\n",
        "    text,stop_words = remove_stop_words(text)\n",
        "    # print(\"All stop_words : \", stop_words)\n",
        "    print(\"## Remove_stop_words from text : \", text)\n",
        "\n",
        "# == Main Run =========================================================\n",
        "input_text = \"Hello, world! This is a sample text with a URL: https://www.myname.com\"\n",
        "print(\"## Raw Text : \", input_text)\n",
        "preprocess_text(input_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2 Write a python code perform stemmer operation using Porterstemmer ,Snowballstemmer,\n",
        "Lancasterstemmer, RegExpStemmer"
      ],
      "metadata": {
        "id": "b1TvgrLrZkEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def stem_with_porter(text): # Simple, Widely used, efficient.\n",
        "    ps = PorterStemmer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def stem_with_snowball(text): # Multilingual, aggressive.\n",
        "    ss = SnowballStemmer(\"english\")\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stemmed_words = [ss.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def stem_with_lancaster(text): # Fast, aggressive.\n",
        "    ls = LancasterStemmer()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stemmed_words = [ls.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def stem_with_regexp(text, regexp):\n",
        "    rs = RegexpStemmer(regexp)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stemmed_words = [rs.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def preprocess_text(input_text):\n",
        "    print(\"Original Text:\", input_text)\n",
        "    words = nltk.word_tokenize(input_text)\n",
        "    porter_stemmed    , snowball_stemmed  = stem_with_porter(input_text)    , stem_with_snowball(input_text)\n",
        "    lancaster_stemmed , regexp_stemmed    = stem_with_lancaster(input_text) , stem_with_regexp(input_text, r'ing$|ed$')\n",
        "    return pd.DataFrame({'Original Word': words, 'Porter': porter_stemmed, 'Snowball': snowball_stemmed, 'Lancaster': lancaster_stemmed, 'RegExp': regexp_stemmed})\n",
        "\n",
        "# == Main Run =========================================================\n",
        "input_text = \"Coders coding coded code\"\n",
        "print(preprocess_text(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa650YLIZkkn",
        "outputId": "8bf5c3d8-eae2-43a7-fb0d-e6eea518c8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Coders coding coded code\n",
            "  Original Word Porter Snowball Lancaster  RegExp\n",
            "0        Coders  coder    coder       cod  Coders\n",
            "1        coding   code     code       cod     cod\n",
            "2         coded   code     code       cod     cod\n",
            "3          code   code     code       cod    code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3 Write a python code to demonstrate the comparative study of all 4 stemmers for a given\n",
        "text corpus."
      ],
      "metadata": {
        "id": "3_mB9NFuZpcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra Testing : only for Snowball with multi language example\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def stem_with_snowball(input_text, _language):\n",
        "    ss = SnowballStemmer(_language)\n",
        "    words = nltk.word_tokenize(input_text)\n",
        "    stemmed_words = [ss.stem(word) for word in words]\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def preprocess_text(input_text):\n",
        "\n",
        "    # Stemming using SnowballStemmer\n",
        "    print(\"\\n\\n## Snowball Stemmed Text (English): \\n\", stem_with_snowball(input_text, \"english\"))\n",
        "    supported_languages = nltk.stem.snowball.SnowballStemmer.languages\n",
        "    print(\"\\nSupported Languages for SnowballStemmers:\")\n",
        "    print(supported_languages)\n",
        "\n",
        "    # \"The children play in the garden while running.\"\n",
        "    french_input_text = \"Les enfants jouent dans le jardin en courant.\"\n",
        "    print(\"\\n\\n\\t\\t## Snowball Stemmed Text (french): \\n\\t\\t(inp)\",french_input_text,\"\\n\\t\\t(out)\", stem_with_snowball(french_input_text, \"french\"))\n",
        "    dutch_input_text = \"De kinderen spelen in de tuin en rennen rond.\"\n",
        "    print(\"\\n\\n\\t\\t## Snowball Stemmed Text (dutch): \\n\\t\\t(inp)\",dutch_input_text,\"\\n\\t\\t(out)\", stem_with_snowball(dutch_input_text, \"dutch\"))\n",
        "\n",
        "# == Main Run =========================================================\n",
        "input_text = \"Coders coding coded code jumping jumped jumps runs running run apples oranges playing played plays\"\n",
        "words = nltk.word_tokenize(input_text)\n",
        "print(\"## Original Text: \\n\", input_text)\n",
        "preprocess_text(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td5jTJmiZqBq",
        "outputId": "656535f0-2010-47c0-a00a-02aa85f8cf79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Original Text: \n",
            " Coders coding coded code jumping jumped jumps runs running run apples oranges playing played plays\n",
            "\n",
            "\n",
            "## Porter Stemmed Text: \n",
            " coder code code code jump jump jump run run run appl orang play play play\n",
            "\n",
            "\n",
            "## Snowball Stemmed Text (English): \n",
            " coder code code code jump jump jump run run run appl orang play play play\n",
            "\n",
            "Supported Languages for SnowballStemmers:\n",
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
            "\n",
            "\n",
            "\t\t## Snowball Stemmed Text (french): \n",
            "\t\t(inp) Les enfants jouent dans le jardin en courant. \n",
            "\t\t(out) le enfant jouent dan le jardin en cour .\n",
            "\n",
            "\n",
            "\t\t## Snowball Stemmed Text (dutch): \n",
            "\t\t(inp) De kinderen spelen in de tuin en rennen rond. \n",
            "\t\t(out) de kinder spel in de tuin en renn rond .\n",
            "\n",
            "\n",
            "## Lancaster Stemmed Text: \n",
            " cod cod cod cod jump jump jump run run run appl orang play play play\n",
            "\n",
            "\n",
            "## RegExp Stemmed Text: \n",
            " Coder cod cod code jump jump jump run runn run apple orange play play play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "import pandas as pd\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def stem_with_porter(words):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(word) for word in words]\n",
        "\n",
        "def stem_with_snowball(input_text, _language):\n",
        "    ss = SnowballStemmer(_language)\n",
        "    words = nltk.word_tokenize(input_text)\n",
        "    return [ss.stem(word) for word in words]\n",
        "\n",
        "def stem_with_lancaster(words):\n",
        "    ls = LancasterStemmer()\n",
        "    return [ls.stem(word) for word in words]\n",
        "\n",
        "def stem_with_regexp(words, regexp):\n",
        "    rs = RegexpStemmer(regexp)\n",
        "    return [rs.stem(word) for word in words]\n",
        "\n",
        "\n",
        "def preprocess_text(input_text):\n",
        "    words = nltk.word_tokenize(input_text)\n",
        "\n",
        "    porter_stemmed = stem_with_porter(words)\n",
        "    snowball_stemmed = stem_with_snowball(input_text, \"english\")\n",
        "    lancaster_stemmed = stem_with_lancaster(words)\n",
        "    regexp_stemmed = stem_with_regexp(words, r'(ing|ed|s)$')\n",
        "\n",
        "    df = pd.DataFrame({'Original Word': words, 'Porter': porter_stemmed, 'Snowball': snowball_stemmed, 'Lancaster': lancaster_stemmed, 'RegExp': regexp_stemmed})\n",
        "    print(df)\n",
        "\n",
        "\n",
        "# == Main Run =========================================================\n",
        "input_text = \"Coders jumping apples oranges playing \"\n",
        "\n",
        "print(\"## Original Text: \", input_text, \"\\n\")\n",
        "\n",
        "preprocess_text(input_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOKtQw5gnq40",
        "outputId": "75f46a04-3455-4ad2-9c98-8ae15908a7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Original Text:  Coders jumping apples oranges playing  \n",
            "\n",
            "  Original Word Porter Snowball Lancaster  RegExp\n",
            "0        Coders  coder    coder       cod   Coder\n",
            "1       jumping   jump     jump      jump    jump\n",
            "2        apples   appl     appl      appl   apple\n",
            "3       oranges  orang    orang     orang  orange\n",
            "4       playing   play     play      play    play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4 Write a python code perform lemmatization using NLTK library."
      ],
      "metadata": {
        "id": "QGHsPHQvZsg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "      lemmas.append(lemmatizer.lemmatize(token))\n",
        "\n",
        "    df = pd.DataFrame({'Raw Word': tokens, '(spaCy)': lemmas})\n",
        "\n",
        "    return df\n",
        "\n",
        "# == Main Run =========================================================\n",
        "text = \"Coders jumping apples oranges playing \"\n",
        "print(lemmatize_text(text))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPwsqQfJZvy_",
        "outputId": "1c4d8d59-d6d5-44a2-dfac-f49e44506063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Raw Word  (spaCy)\n",
            "0   Coders   Coders\n",
            "1  jumping  jumping\n",
            "2   apples    apple\n",
            "3  oranges   orange\n",
            "4  playing  playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5 Write a python code perform lemmatization using Spacy library."
      ],
      "metadata": {
        "id": "NzH-Ml5bZsO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "      lemmas.append(token.lemma_)\n",
        "    df = pd.DataFrame({'Raw Word': doc, '(spaCy)': lemmas})\n",
        "    return df\n",
        "\n",
        "# == Main Run =========================================================\n",
        "text = \"Coders jumping apples oranges playing \"\n",
        "print(lemmatize_text(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy-bIWOtZtzB",
        "outputId": "d0b4c836-e98e-485a-ce65-16fb63d8cfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Raw Word (spaCy)\n",
            "0   Coders   coder\n",
            "1  jumping    jump\n",
            "2   apples   apple\n",
            "3  oranges  orange\n",
            "4  playing    play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6 Compare the results lemmatization with Spacy and NLTK for the corpus given below-\n",
        "walking, is , main, animals , foxes, are, jumping , sleeping."
      ],
      "metadata": {
        "id": "J3rc_j4EZ210"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_text_nltk(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "      lemmas.append(lemmatizer.lemmatize(token))\n",
        "    return lemmas\n",
        "\n",
        "def lemmatize_text_spacy(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "      lemmas.append(token.lemma_)\n",
        "    return lemmas\n",
        "\n",
        "# == Main Run =========================================================\n",
        "text = \"Coders jumping apples oranges playing \"\n",
        "tokens = word_tokenize(text)\n",
        "Lemmatized_text_NLTK = lemmatize_text_nltk(text)\n",
        "Lemmatized_text_spaCy = lemmatize_text_spacy(text)\n",
        "\n",
        "df = pd.DataFrame({'Raw Word': tokens, '(NLTK)': Lemmatized_text_NLTK, '(spaCy)': Lemmatized_text_spaCy})\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk65GkzwZ3p3",
        "outputId": "e50db2ba-b626-4422-8053-94ea011e543f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Raw Word   (NLTK) (spaCy)\n",
            "0   Coders   Coders   coder\n",
            "1  jumping  jumping    jump\n",
            "2   apples    apple   apple\n",
            "3  oranges   orange  orange\n",
            "4  playing  playing    play\n"
          ]
        }
      ]
    }
  ]
}